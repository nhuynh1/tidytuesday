---
title: "Ramen Ratings"
author: "Nancy Huynh"
date: '2019-06-04'
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ramen Ratings from The Ramen Rater
This week I take a stab at LASSO regression using the week 23  [#TidyTuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-06-04) data. The original data comes from [The Ramen Rater](https://www.theramenrater.com/resources-2/the-list/), which is indeed ratings for ramen (the "instant noodle" type). I first saw LASSO regression from [Dave Robinson's YouTube screencast](https://www.youtube.com/watch?v=qirKGdQvy9U) of his analysis on a TidyTuesday dataset earlier this year. Other resources I used are below.

## Data Import and Libraries
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(glmnet)
library(broom)

ramen_ratings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-04/ramen_ratings.csv", col_types = "ifcffd")

summary(ramen_ratings)

```

```{r echo=FALSE, include=FALSE}
## my theme object from previous TidyTuesday
nh_theme <- theme(panel.background = element_rect(fill = "white"),
          strip.background = element_blank(),
          strip.text = element_text(hjust = 0, size = 12, color = "grey25", face = "bold"),
          panel.spacing = unit(2, "lines"),
          axis.line = element_line(color = "grey85"),
          panel.grid.major.y = element_line(color = "grey75", linetype = "dotted"),
          legend.key = element_blank(),
          legend.key.height = unit(1, "cm"),
          legend.title = element_text(size = "10"),
          plot.title = element_text(size = 15, color = "grey15", face = "bold"),
          plot.subtitle = element_text(size = 12, color = "grey30"),
          text = element_text(color = "grey40"))
```

## What are the ratings across brand, style, and country of origin?
Since there are numerous countries and brands I use `fct_lump` to grab the top 10 and lump the remaining into other.
```{r ramen_boxplots}
ramen_boxplots <- function(ramen_df, ramen_col) {
  ramen_col <- enquo(ramen_col)
  
  ramen_df %>%
    ggplot(aes(x = fct_relevel(fct_reorder(fct_lump(!!ramen_col, n = 10), stars, na.rm = TRUE), "Other", after = 0), y = stars)) +
    geom_boxplot(na.rm = TRUE) + 
    coord_flip() +
    labs(y = "Star Rating",
         x = str_to_title(rlang::as_label(ramen_col)))
}

ramen_boxplots(ramen_ratings, brand)
ramen_boxplots(ramen_ratings, country)
ramen_boxplots(ramen_ratings, style)
```

## What factors influences the star rating of the ramen?
Going to try a lasso regression without engineering any extra features. But, some ideas for engineered features from the `variety` variable could be spicy (i.e. words like "Spicy", "Chili") or noodle type (i.e. words like "udon"", "vermicelli").
```{r ramen_lasso}

#training data set
ramen_train  <- ramen_ratings %>%
  sample_frac(0.80) %>%
  na.omit()

#testing data set
ramen_test <- ramen_ratings %>%
  anti_join(ramen_train, by = "review_number") %>%
  na.omit()

#create matrix of predictors
features <- model.matrix(stars ~ brand + style + country, ramen_train)[, -1]

#determine which value of lambda to use
cv.lasso <- cv.glmnet(x = features, y = ramen_train$stars, alpha = 1)
plot(cv.lasso)

cv.lasso$lambda.min
cv.lasso$lambda.1se

#choosing lambda.1se as this seems to be the typical choice so as to not overfit
ramen_coefs <- cv.lasso$glmnet.fit %>%
  tidy() %>%
  filter(lambda == cv.lasso$lambda.1se) %>%
  arrange(desc(estimate))
```

```{r ramen_coefs}
ramen_coefs %>%
  filter(term != "(Intercept)") %>%
  top_n(30, abs(estimate)) %>%
  mutate(term = str_replace(term, "brand", "brand: "),
         term = str_replace(term, "country", "country: "),
         term = str_replace(term, "style", "style: ")) %>%
  ggplot(aes(x = fct_reorder(term, -estimate), y = estimate)) +
  geom_col() +
  coord_flip() +
  labs(y = "Coefficient",
       x = "",
       title = "Largest coefficients to predict ramen ratings",
       subtitle = "Based on LASSO regression",
       caption = "Data: The Ramen Rater \n @nh_writes") +
  nh_theme

#ggsave("ramen_coefficients.png", device = "png", units = "cm", width = 29, height = 21, dpi = "retina")
```

### Testing the model with the test set `ramen_test`
Since the model predicts stars as a continous number, but the ramen rater rates in 0.25 increments so I rounded the predicted values to the nearest 0.25.

#### LASSO model with lambda.1se
```{r}

ramen_lasso_model <- glmnet(x = features, y = ramen_train$stars, alpha = 1, lambda = cv.lasso$lambda.1se)
ramen_test_features <- model.matrix(stars ~ brand + style + country, ramen_test)[, -1]

predicted_stars <- as.data.frame(predict(ramen_lasso_model, newx = ramen_test_features)) %>%
  rename(predicted = s0) %>%
  bind_cols(stars = ramen_test$stars) %>%
  mutate(predicted_stars = round(predicted * 4) / 4,
         error2 = (predicted_stars - stars)^2)

predicted_stars %>%
  summarise(RMSE = sqrt(mean(error2)))

```

#### LASSO model with lambda.min
```{r}

ramen_lasso_model <- glmnet(x = features, y = ramen_train$stars, alpha = 1, lambda = cv.lasso$lambda.min)
ramen_test_features <- model.matrix(stars ~ brand + style + country, ramen_test)[, -1]

predicted_stars <- as.data.frame(predict(ramen_lasso_model, newx = ramen_test_features)) %>%
  rename(predicted = s0) %>%
  bind_cols(stars = ramen_test$stars) %>%
  mutate(predicted_stars = round(predicted * 4) / 4,
         error2 = (predicted_stars - stars)^2)

predicted_stars %>%
  summarise(RMSE = sqrt(mean(error2)))

```

#### Linear model
I hade trouble running the model against the testing data `ramen_test` because some of the brands in that set were not present in the training set so I ran `predict` on the same training data set to get the error and RMSE. Since the model is fitted to that data it is expected that the RMSE would be lower than the LASSO models.
```{r}

ramen_lm_model <- lm(stars ~ brand + style + country, data = ramen_train)
predicted_stars <- data.frame(predicted = predict.lm(ramen_lm_model)) %>%
  bind_cols(stars = ramen_train$stars) %>%
  mutate(predicted_stars = round(predicted * 4) / 4,
         error2 = (predicted_stars - stars)^2)

predicted_stars %>%
  summarise(RMSE = sqrt(mean(error2)))
```

After removing the brands in the test set that were not present in the training set the RMSE is much higher than the linear RMSE above and is very close to the two LASSO model RMSEs. Since LASSO models are less complex, this bodes well as simpler models with similar error is generally considered better from a practical standpoint.
```{r}

ramen_test_brands_rm <- semi_join(ramen_test, ramen_train, by = "brand")
predicted_stars <- data.frame(predicted = predict.lm(ramen_lm_model, newdata = ramen_test_brands_rm)) %>%
  bind_cols(stars = ramen_test_brands_rm$stars) %>%
  mutate(predicted_stars = round(predicted * 4) / 4,
         error2 = (predicted_stars - stars)^2)

predicted_stars %>%
  summarise(RMSE = sqrt(mean(error2)))

```


## Things to do later
This is by no means a complete analysis; I'm just trying out LASSO regression using this fairly simple dataset. After looking at the top 30 coefficients I should somehow limit the brands to those that occur more than say 5 times. For example the largest coefficient is brand "Roland", which only occurs 2 times in the whole dataset.

## References
* [Lasso Regression: Simple Definition](https://www.statisticshowto.datasciencecentral.com/lasso-regression/)
* [Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/)
